\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{tabularx} % Per tabelle a larghezza fissa
\usepackage{booktabs} % Per linee professionali (toprule, midrule, bottomrule)
\usepackage[table,dvipsnames]{xcolor} % Per colorare le righe
\usepackage[export]{adjustbox}
\usepackage{hyperref}

\title{Soft17 -- An Intelligent Agent for Blackjack}
\author{Abbatiello Simone, Nappi Vincenzo, Niemiec Francesco}
\date{January 2026}

\begin{document}

\maketitle

\section{Introduzione}

\subsection{Genesi del progetto}

Durante la seconda parte del corso di \textit{Fondamenti di Intelligenza Artificiale}, in particolare quella dedicata agli argomenti di \textbf{Machine Learning}, è emerso in modo sempre più evidente il forte legame tra le tecniche di apprendimento automatico e il mondo della statistica.  

Abbiamo deciso di cogliere l’occasione offerta dallo sviluppo del progetto finale per approfondire tale correlazione, analizzandola in un contesto applicativo concreto. In seguito a una fase di brainstorming, volta a individuare un dominio che consentisse di unire questi due ambiti, abbiamo preso in considerazione i documenti forniti dal docente, nei quali il tema dei \textit{giochi} emerge come un caso di studio ricorrente.  

Guidati dalla curiosità e dalla relativa semplicità delle regole, abbiamo quindi scelto il gioco del \textbf{Blackjack}. Da questa scelta nasce l’idea di sviluppare un modello che sia in grado di stimare l’esito atteso di una mano di Blackjack sulla base delle informazioni disponibili e delle strategie tipiche.



\section{Nozioni basilari di Blackjack}
\subsection{Overview}
Il Blackjack è un gioco di carte dalla forte semplicità, non a caso è tra i più diffusi nei casinò di tutto il mondo.
Nella sua versione comune il prevede l'uso di un numero di mazzi standard di 52 carte francesi che va da 1 a 8, e coinvolge solo due tipologie di figure: il "banco" (anche detto "dealer") ed i giocatori. L’obiettivo del singolo giocatore è ottenere una mano il cui valore totale sia pari o quanto più possibile vicino a 21, senza superare tale soglia, e che risulti superiore a quello del banco. Il banco gioca secondo le stesse regole e di conseguenza perde nel momento in cui totalizza un valore minore del giocatore o quando esso sfora la soglia del ventuno. Le carte numeriche mantengono il loro normale valore, le figure valgono ciascuna 10 punti, mentre l’asso può assumere valore 1 o 11 a seconda della scelta del giocatore. 
Il gioco prende il nome dalla combinazione delle carte asso e jack nero, ma nella sua terminologia viene chiamata blackjack qualunque combinazione di un asso con una figura. Ogni partecipante compete esclusivamente contro il banco, non contro gli altri giocatori al tavolo, e può compiere una serie di scelte (come chiedere carta, stare, raddoppiare o dividere) che incidono direttamente sull’esito della mano. Ciò nonostante nella sua variante a più giocatori nel momento in cui il dealer sfora, tutti i giocatori che non sforano vincono, avendo ognuno di fatto un'influenza, anche se implicita su ciascuna partita.
La situazione iniziale di ogni partita è costituita dal dealer con due carte: una scoperta ed una coperta, ed il giocatore sulla base delle due carte che riceve deve stabilire se pescare ancora (\textbf{HIT}) o rimanere con solo quelle due carte (\textbf{STAND}).
Vi sono anche ulteriori possibilità, lo \textbf{SPLIT} può verificarsi quando vengono pescate come prime due carte dal giocatore due carte dallo stesso valore, in tal caso è possibile considerarle come due "prime carte" e giocare due mani contemporaneamente.
L'ultimo caso è il \textbf{DOUBLE} nel quale si va a raddoppiare la puntata iniziale.

Un elemento distintivo del blackjack rispetto ad altri giochi da casinò è la possibilità di intervento razionale del giocatore sull’andamento della partita. Sebbene il gioco resti fondamentalmente basato sul caso, esistono strategie matematicamente fondate, in grado di aiutare il giocatore.
Proprio quest'ultimo aspetto matematico ha portato moltissima attenzione dalla comunità scientifica sul gioco,
aprendo le porte prima ad analisi statistiche e poi a nuovi orizzonti come algoritmi genetici e modelli di Machine Learning.
\subsection{Vantaggio del banco}
Chiamiamo vantaggio del banco la misura quantitativa dello svantaggio strutturale a cui è sottoposto il giocatore di blackjack nel lungo periodo. Esso esprime la perdita media attesa per unità di puntata ed è un indicatore fondamentale per l’analisi matematica ed economica del gioco, in quanto sintetizza l’effetto congiunto delle regole, delle probabilità e dell’ordine delle decisioni.

A differenza di altri giochi da casinò, il vantaggio del banco nel blackjack non è fisso, ma dipende in modo significativo dalla configurazione regolamentare del tavolo e dal comportamento del giocatore.
L’origine del vantaggio del banco è riconducibile a una serie di asimmetrie strutturali.
La più rilevante è l’ordine di gioco: il giocatore è obbligato ad agire per primo e perde immediatamente la puntata nel caso in cui superi 21, indipendentemente dal risultato finale del banco. Questa regola, apparentemente neutrale, introduce un vantaggio matematico stabile a favore del casinò. A ciò si aggiungono le regole fisse del banco, che ne limitano la discrezionalità ma risultano, nel complesso, statisticamente ottimali.

Ulteriori contributi al vantaggio del banco derivano dalla struttura dei pagamenti. Il pagamento del blackjack naturale è un elemento cruciale: una remunerazione pari a 3:2 riduce significativamente il margine del casinò, mentre formule meno favorevoli al giocatore, come il pagamento 6:5, possono incrementare il vantaggio del banco. 
Anche il numero di mazzi utilizzati e le restrizioni sulle opzioni di gioco (raddoppio, divisione, resa) incidono in modo misurabile sul margine complessivo.

Dal punto di vista teorico, il vantaggio del banco nel blackjack è di particolare interesse perché non è imposto esclusivamente dal caso, ma emerge dall’interazione tra regole istituzionali e decisioni individuali. Esso rappresenta quindi un esempio di come un sistema apparentemente equo possa generare, attraverso vincoli asimmetrici, un esito statisticamente sfavorevole a una delle parti.

Attraverso l'attuazione di alcuni comportamenti è possibile ridurre questo svantaggio, primo di tutti è l'applicazione della strategia di base.

\subsection{Strategia di base}
Parliamo di strategia di base come dell’insieme di decisioni ottimali che il giocatore dovrebbe adottare in ogni possibile configurazione della propria mano e della carta scoperta del banco, assumendo condizioni di gioco standard e l’assenza di informazioni supplementari sull’ordine delle carte. Essa costituisce il riferimento teorico fondamentale per l’analisi razionale del gioco e per la valutazione del vantaggio statistico del banco.
Per ciascuna combinazione di mano del giocatore e carta visibile del banco, viene determinata l’azione che massimizza il valore atteso della giocata.
È importante effettuare una distinzione tra \textbf{hard-hand}, in cui l’asso ha valore fisso pari a 1, e \textbf{soft-hand}, in cui l’asso può assumere valore 11 senza rischio immediato di sballo.
L’adozione coerente della strategia di base consente di ridurre il margine di vantaggio del banco a valori minimi, che in condizioni regolamentari favorevoli possono scendere a meno del'uno percento. 
È tuttavia importante sottolineare che la strategia di base non elimina il rischio né garantisce vincite nel breve periodo: essa opera esclusivamente sul piano statistico, ottimizzando le decisioni nel lungo termine.
Nel dettaglio questa strategia è stata formalizzata nel 1956 da Roger R. Baldwin, nell'articolo "The Optimal Strategy in Blackjack". Di seguito riportiamo le tabelle relative alla strategia di base del Blackjack.

---PLACEHOLDER---

\section{Descrizione dell’agente}

\subsection{Obiettivi di business}

L’obiettivo del progetto è lo sviluppo di un \textbf{agente intelligente} in grado di:

\begin{itemize}
    \item selezionare, per ogni mano osservabile, l’azione più appropriata al fine di massimizzare il \textit{win rate}. Le azioni considerate sono:
    \begin{itemize}
        \item \textbf{Hit}: richiedere una carta aggiuntiva;
        \item \textbf{Stand}: mantenere la mano attuale;
        \item \textbf{Double}: raddoppiare la puntata iniziale e ricevere una sola carta; //Da togliere 
        \item \textbf{Split}: dividere la mano in due mani distinte in presenza di due carte uguali.
    \end{itemize}
    \item prendere decisioni esclusivamente sulla base della propria mano e della carta scoperta del dealer, senza conoscere le carte rimanenti nel mazzo;
    \item migliorare le proprie prestazioni nel tempo, apprendendo dai dati osservati.
\end{itemize}

\subsection{Specifica dell’ambiente (PEAS)}

L’ambiente in cui opera l’agente viene descritto attraverso la specifica \textbf{PEAS} (Performance, Environment, Actuators, Sensors).

\begin{description}
    \item[Performance:] la misura di prestazione dell’agente è legata alla sua capacità di massimizzare il risultato economico nel gioco. In particolare vengono considerate nelle prime due pipeline l'accuratezza e robustezza nella previsione dell’esito della mano (vittoria o non sconfitta) a partire dallo stato informativo disponibile al momento della decisione, valutate tramite metriche di classificazione supervisionata (F1-score come metrica principale, affiancata da precision e recall). Nell'ultima pipeline invece andiamo a considerare la percentuale di vittorie dell'agente.
    
    \item[Environment:] l’ambiente è rappresentato da un tavolo di Blackjack comprendente:
    \begin{itemize}
        \item uno o più mazzi di carte con distribuzione stocastica;
        \item un dealer che segue una policy fissa;
        \item la mano del giocatore;
        \item l’insieme delle regole del gioco.
    \end{itemize}
    Le proprietà dell’ambiente sono:
    \begin{itemize}
        \item \textbf{Parzialmente osservabile}: l’agente conosce solo la propria mano e la carta scoperta del dealer;
        \item \textbf{Stocastico}: l’ordine delle carte nel mazzo è pseudocasuale;
        \item \textbf{Sequenziale}: ogni azione influenza l’esito finale della mano;
        \item \textbf{Statico}: l’ambiente non cambia mentre l’agente sta deliberando;
        \item \textbf{Discreto}: il numero di stati e azioni è finito;
        \item \textbf{A singolo agente}: il dealer segue una strategia prefissata e non è considerato un agente decisionale.
    \end{itemize}

    \item[Actuators:] gli attuatori dell’agente corrispondono alle azioni disponibili: Hit, Stand, Double e Split.

    \item[Sensors:] l’agente percepisce:
    \begin{itemize}
        \item le carte della propria mano;
        \item la carta scoperta del dealer;
        \item le azioni consentite in un determinato stato (ad esempio la possibilità di effettuare double o split);
        \item l’esito finale della mano (vittoria, sconfitta o pareggio).
    \end{itemize}
\end{description}

\subsection{Scelta della strategia}

Un primo approccio al problema avrebbe potuto basarsi sulla teoria dei giochi e sull’utilizzo di algoritmi genetici. Tuttavia, tale soluzione presenta diverse criticità:

\begin{description}
    \item[Variabilità dei cromosomi:] la rappresentazione delle soluzioni risulterebbe altamente variabile, rendendo complessa la codifica delle strategie.
    \item[Complessità dello spazio di ricerca:] l’elevato numero di stati e azioni comporta uno spazio di ricerca estremamente ampio, con conseguenti tempi di elaborazione proibitivi.
    \item[Inaccuratezza della fitness:] la natura stocastica dell’ambiente rende la funzione di fitness fortemente influenzata dalla casualità.
\end{description}

Alla luce di queste considerazioni, abbiamo deciso di affrontare il problema tramite tecniche di \textbf{apprendimento supervisionato}. In questo contesto, l’agente apprende una funzione che associa a ogni stato osservabile del gioco un’etichetta nota, rappresentata dall’esito della mano, utilizzando un dataset di mani simulate.

\section{Metodologia e Raccolta dati}

\subsection{Metodologia di sviluppo: CRISP-DM}

Lo sviluppo del progetto è stato guidato dal modello CRISP-DM (Cross-Industry Standard Process for Data Mining), un processo standard ampiamente adottato per la realizzazione di sistemi di data mining e machine learning. Tale modello fornisce una visione strutturata e iterativa del ciclo di vita di un progetto di apprendimento automatico, suddividendolo in fasi ben definite.

In particolare, il CRISP-DM prevede le seguenti fasi principali:
\begin{itemize}
    \item \textbf{Business Understanding}: definizione degli obiettivi del sistema e dei criteri di successo;
    \item \textbf{Data Understanding}: raccolta dei dati, analisi esplorativa e valutazione della loro qualità;
    \item \textbf{Data Preparation}: pulizia dei dati e costruzione delle feature rilevanti;
    \item \textbf{Modeling}: selezione e addestramento dei modelli di apprendimento automatico;
    \item \textbf{Evaluation}: valutazione delle prestazioni del modello rispetto agli obiettivi prefissati;
    \item \textbf{Deployment}: integrazione del modello all’interno di un sistema utilizzabile.
\end{itemize}

Nel contesto del presente lavoro, le fasi di \emph{Business Understanding} e \emph{Data Understanding} sono state affrontate attraverso la definizione degli obiettivi dell’agente intelligente e l’analisi preliminare dei dati generati dal simulatore di Blackjack. Le successive operazioni di data cleaning e feature engineering rientrano nella fase di \emph{Data Preparation}, il cui obiettivo è rendere i dati idonei all’addestramento di modelli di apprendimento supervisionato.

In conclusione, il modello CRISP-DM è stato utilizzato come \textbf{riferimento metodologico} per strutturare il progetto, senza l’obiettivo di coprirne esplicitamente tutte le fasi, ma adottandone i principi per guidare le scelte progettuali effettuate.

\subsection{Scelta del dataset}

Per la scelta del dataset sono state considerate due possibili strategie:
\begin{enumerate}
    \item Generare un dataset sintetico tramite un simulatore di Blackjack, in grado di produrre un numero arbitrario di mani realistiche.
    
    \item Utilizzare dataset già disponibili online (\url{https://www.kaggle.com/datasets/mojocolors/900000-hands-of-blackjack-results}) e (\url{https://www.kaggle.com/datasets/dennisho/blackjack-hands/data}).
\end{enumerate}

La prima opzione come pro avrebbe garantito un grande controllo sui dati, ma un errore minimo nel simulatore avrebbe o una nostra scelta sbagliata e/o inesperta nella categorizzazione dei dati avrebbe avuto un impatto troppo impatto sul modello.
La seconda opzione seppur all'apparenza più facile, avrebbe potuto celare delle insidie, come una difficile comprensione dei dataset oppure "fastidi" dovuti alla loro enorme dimensione. 
Avendo valutato per bene ambo le alternativi ci siamo trovati inizialmente di fronte ad un bivio, successivamente abbiamo optato per una soluzione "ibrida".
Il più corposo tra i principali due dataset trovati contava cinquanta milioni di mani di blackjack simulate, un numero talmente elevato che il training del modello con le risorse a nostra disposizione sarebbe risultato infito. Esso però riportava anche una repository \textbf{GitHub} con il programma usato per simulare queste mani.
Abbiamo quindi preso questo programma e generato un dataset con circa cinquecentomila mani giocate, una quantità tale da poter effettuare un buon training in tempi tutto sommato contenuti.
È importante notare come la scelta di mantenere inalterato l'algoritmo nasce da due ragioni: la prima mantenere inalterati gli ottimi risultati relativi all'usabilità del dataset, la seconda (nonchè forse la più importante) è quella di voler andare ad operare direttamente sui dati anzichè sulla loro generazione.

\subsection{Regole seguite}
Per generare il dataset si è quindi scelto di utilizzare le stesse regole indicate dal dataset (\url{https://www.kaggle.com/datasets/dennisho/blackjack-hands/data}).
Le regole in questione sono:
\begin{itemize}
    \item \textbf{Shoe da 8 mazzi} (penetrazione di 6,5 mazzi)
    \item La prima carta dello shoe viene scartata
    \item Il Blackjack paga 3:2
    \item È possibile raddoppiare su qualsiasi combinazione iniziale di due carte
    \item Il raddoppio dopo lo split è consentito
    \item È possibile splittare qualsiasi coppia iniziale fino a un massimo di 4 mani
    \item Non è consentito il re-split degli Assi
    \item Dopo lo split degli Assi viene distribuita una sola carta per mano e non è possibile ottenere Blackjack
    \item Non è consentita la resa dopo uno split
    \item Il banco pesca su soft 17 (H17)
\end{itemize}
Bisogna notare anche quanto segue:
\begin{itemize}
    \item Tutte le mani sono giocate \textbf{heads-up} (un solo giocatore contro il banco)
    \item La puntata iniziale per ogni mano è sempre pari a 1
    \item Le carte 10, J, Q e K sono considerate equivalenti e registrate come 10
    \item Gli Assi sono sempre registrati come 11, indipendentemente dal loro valore effettivo nella mano
    \item I semi non vengono considerati e non sono registrati
    \item Il \textbf{Running Count} e il \textbf{True Count} (troncati all’intero) sono calcolati con il sistema \textbf{Hi-Lo} e registrati all’inizio di ogni mano, prima della distribuzione delle carte
    \item Il numero di carte rimanenti nello shoe è registrato all’inizio di ogni mano e include anche le carte che non verranno utilizzate a causa di una penetrazione inferiore al cento percento
\end{itemize}


\section{Prima pipeline}
\subsection{Esplorazione}
Il dataset generato necessità di varie operazioni di pulizia dei dati ed in generale di data engineering per renderlo consono ai nostri scopi.
C'è necessità di \textbf{Feature scaling}, \textbf{Feature construction}, \textbf{Feature extraction}. 
Iniziamo però da una fase iniziale di esplorazione dei dati, la quale serve per comprendere su quali aree andare ad agire.
Data la natura condivisa di questa fase abbiamo deciso di usare \textit{Colab}(quindi i comandi che andremo a specificare successivamente faranno riferimento alla sua sintassi).
Come prima cosa quindi abbiamo importato il file del dataset (in formato .csv) e abbiamo visualizzato le sue colonne. In particolare il dataset iniziale era composto da queste colonne: 
\begin{itemize}
    \item \textbf{shoe\_id}: Identificativo del mazzo utilizzato per la mano.
    \item \textbf{cards\_remaining}: Numero di carte rimanenti nel mazzo all'inizio della mano.
    \item \textbf{dealer\_up}: Carta scoperta del dealer.
    \item \textbf{initial\_hand}: Mano iniziale del giocatore.
    \item \textbf{dealer\_final}: Mano finale del dealer.
    \item \textbf{dealer\_final\_value}: Valore finale della mano del banco.
    \item \textbf{player\_final}: Mano finale del giocatore dopo tutte le azioni.
    \item \textbf{player\_final\_value}: Valore finale della mano del giocatore.
    \item \textbf{actions\_taken}: Sequenza di azioni effettuate dal giocatore.
    \item \textbf{run\_count}: Numero di round giocati nella simulazione.
    \item \textbf{true\_count}: Conteggio all'inizio della mano.
    \item \textbf{win}: Risultato della mano per il giocatore.
    \end{itemize}
Successivamente abbiamo individuato \textit{WIN} come colonna utile per la nostra classificazione, di fatto abbiamo analizzato quali fossero i valori che potesse assumere e li abbiamo raggruppati in tre categorie, ovvero minori di zero, uguali a zero e maggiori di zero.
Fatto ciò abbiamo visualizzato il numero di istanze per ciascuna categoria:
- 421940 istanze con win maggiore di zero.
- 493992 istanze con win minore di zero.
- 84068 istanze con win uguale a zero.
A primo impatto il dataset presentava uno sbilanciamento verso le "sconfitte", tuttavia abbiamo scelto di non considerarlo alla luce di due considerazioni: la prima è che è inerente nella natura del gioco (favorevole al banco) che siano presenti molte più sconfitte che vittorie, e la seconda è che andando a considerare semplicemente i casi "favorevoli" ovvero quelli dove la win è maggiore o uguale a zero, il dataset sarebbe apparso come bilanciato.
Successivamente abbiamo verificato quanti valori nulli fossero presenti nelle colonne, i risultati hanno mostrato che nessuna colonna presentava valori nulli. Ciò non era assolutamente inaspettato data la natura "sintetica" dei dati.
\subsection{Data Cleaning}
Siccome tutte le colonne del dataset non presentano dati mancanti, questa fase non comprende all'atto pratico nessun cambiamento.
\subsection{Feature Scaling}
Avendo precedentemente specificato che non vogliamo concentrarci sull'aspetto economico delle singole puntate, ci viene naturale
il voler normalizzare i valori di win per identificare semplicemente i tre casi possibili:
\begin{itemize}
    \item Vittoria (attualmente 0 < x <= 7, vorremmo solamente 1)
    \item Pareggio (attualmente == 0)
    \item Sconfitta (attualmente da < 0 a -7, vorremmo solamente -1)
\end{itemize}
Per fare ciò abbiamo valutato diverse opzioni:
\begin{description}
    \item[Normalizzazione min-max:] il problema di questo tipo di normalizzazione era che a noi interessava mantenere anche lo zero a fini statistici ma applicandola ciò non sarebbe possibile (se invece avessimo considerato lo 0 come parte della vittoria allora sarebbe stata perfetta)
    \item [Normalizzazione Z-score:] qui non avremmo avuto pieno controllo ma la trasformazione sarebbe stata attuata sulla densità media.
    \end{description}
Abbiamo quindi optato per una strategia di "Discretizzazione con soglie"(\textbf{da aggiungere nel glossario}) che ci permette di fare esattamente ciò che
ci serve.
\subsection{Feature Engineering}
Nonostante il lavoro effettuato nella fase precedente, il dataset risultava fortemente inadeguato ai nostri scopi, quindi in questa fase tramite tecniche di Feature Selection e Feature Construction abbiamo cercato di plasmarlo al meglio per i nostri scopi.
\subsubsection{Feature Selection}
Iniziamo da ciò che siamo andati a rimuovere.
Primo step è stato quello di eliminare shoe\_id in quanto è un semplice indicatore del mazzo di carte e non ha alcuna correlazione con la decisione nel nostro contesto.
Fatto ciò passiamo all'eliminazione di colonne che potrebbero causare \textbf{Data Leakage}, in particolare facciamo riferimento
a:
\begin{itemize}
    \item \textbf{dealer\_final}: rappresenta la mano finale del dealer e, di conseguenza, non è un'informazione disponibile al momento della decisione del giocatore.
    \item \textbf{dealer\_final\_value}: valore finale della mano del dealer; per le stesse motivazioni espresse sopra, questa informazione è osservabile solo a fine mano.    
    \item \textbf{player\_final}: descrive la mano finale del giocatore, includendo carte ottenute attraverso azioni future non ancora osservabili nel momento della decisione.
    \item \textbf{player\_final\_value}: valore finale della mano del giocatore; analogamente alla variabile precedente, non è disponibile prima del completamento della sequenza di azioni.
    \item \textbf{actions\_taken}: rappresenta un vero e proprio caso di \emph{data leakage}, in quanto codifica direttamente la sequenza di azioni che il modello dovrebbe invece apprendere a prevedere.
\end{itemize}
Detto ciò abbiamo eliminato anche run\_count, true\_count e cards\_remaining perchè relativi al conteggio delle carte.
\subsubsection{Feature Construction}
Una volta conclusa la fase di \emph{Feature Selection}, è stato necessario modificare alcune colonne del dataset al fine di renderle utilizzabili dal modello.
La prima trasformazione riguarda la variabile \textbf{initial\_hand}, che è rappresentata come una lista di carte. Questa rappresentazione risulta poco adatta all’utilizzo diretto da parte del modello; per questo motivo si è deciso di scomporla in tre differenti variabili:
\begin{itemize}
    \item \textbf{player\_sum}: la somma dei valori di tutte le carte presenti nella mano del giocatore
    \item \textbf{player\_is\_soft}: variabile binaria che indica se la mano è \emph{soft}, ovvero se contiene un asso valutabile come 11
    \item \textbf{player\_pair}: variabile binaria che indica se la mano è composta da due carte dello stesso valore e quindi se è possibile effettuare lo \emph{split}.
\end{itemize}
\subsection{Colonne finali del Dataset}
Al termine della fase di elaborazione, il dataset finale risulta composto dalle seguenti variabili:
\begin{itemize}
    \item \textbf{player\_sum}
    \item \textbf{player\_is\_soft}
    \item \textbf{player\_pair}
    \item \textbf{dealer\_up}
\end{itemize}
La variabile target è \textbf{win}, che indica l’esito della decisione presa dal giocatore, assumendo i valori di vittoria e sconfitta.
\subsection{Modeling}
Prima di parlare degli algoritmi utilizzati occorre fare delle premesse. Per scegliere i migliori parametri adatti agli algoritmi e il problema che stiamo affrontando abbiamo deciso di utilizzare una \textbf{griglia di ricerca} per identificare gli iperparametri. È una classe contenuta nella libreria scikit-learn che ci permette di testare più combinazioni di parametri automaticamente evitando di eseguire a mano i training.
La prima operazione effettuata è lo splitting del dataset per distinguere i dati di test e i dati per il training. Nello specifico l'80\% dei dati sarà utilizzato per il training e il 20\% per il testing. Avviene successivamente un ulteriore split dei dati di training distinguendo tra training e validazione, nello specifico il 78.5\%  per il training e il restante per la validazione. È stato inoltre fissato un seme randomico per rendere i test ripetibili. % Da aggiustare%
Infine stratify permette di suddividere il dataset in modo che ogni sottoinsieme mantenga la stessa identica proporzione di classi del dataset originale.
Per la prima versione di questa pipeline (Pipeline 1A) l'algoritmo utilizzato è un albero decisionale di classificazione (DecisionTreeClassifier) specificando i seguenti parametri: 

\begin{itemize}
    \item \textbf{ max\_depth:} che specifica la profondità massima che l'albero può raggiungere con i seguenti valori: [10,20,30]
    \item \textbf{ min\_samples\_split:} che determina numero minimo di campioni prima di effettuare un'ulteriore ramificazione. I valori sono: [2,5]
    \item \textbf{ min\_samples\_leaf:} che determina il numero minimo di campioni che devono essere presenti nelle foglie. I valori sono: [1,2]
\end{itemize}
\subsection{Evaluation}
I migliori parametri scelti dalla griglia sono: max\_depth = 10, 'min\_samples\_leaf'= 1, 'min\_samples\_split'= 2 .
Le metriche invece ci hanno fornito i seguenti risultati: 
  F1-Score Test: 0.6562
  Accuracy:  0.6844
  Precision:  0.7268
  Recall:  0.5982
  \begin{center}
      \includegraphics[scale=0.8, center]{MatriceDiConfusionePipeline1.png}
  \end{center}
\section{Seconda pipeline}
Per la seconda versione della pipeline (Pipeline 1B) è stato adottato un
approccio basato su \textbf{ensemble di alberi decisionali, ovvero Random Forest}
(\textit{RandomForestClassifier}), mantenendo invariate tutte le fasi di
preprocessing, feature engineering e suddivisione del dataset rispetto alla
pipeline 1A, al fine di garantire un confronto equo tra i modelli.

Analogamente a quanto fatto per la pipeline 1A, anche in questo caso è stato
utilizzata una \textbf{griglia di ricerca} per la selezione degli iperparametri, operando
sulla medesima formulazione del problema come task di classificazione binaria.

In particolare:
\begin{itemize}
    \item vengono utilizzate le stesse feature;
    \item non vengono introdotte ulteriori trasformazioni dei dati;
    \item il processo di selezione degli iperparametri è identico a quello
    adottato per la pipeline 1A.
\end{itemize}
I principali iperparametri considerati per la Random Forest sono:
\begin{itemize}
    \item \textbf{n\_estimators:} numero di alberi presenti.
    I valori testati sono: [100, 200];
    \item \textbf{max\_depth:} profondità massima consentita per ciascun albero.
    I valori considerati sono: [15, 20];
    \item \textbf{min\_samples\_split:} numero minimo di campioni richiesto per
    effettuare una suddivisione interna. I valori testati sono: [2, 5].
\end{itemize}

L’introduzione di questa pipeline consente di valutare l’impatto di un modello
più complesso rispetto a un singolo albero decisionale sul problema considerato.
Dai risultati sperimentali emerge che la pipeline 1B non fornisce miglioramenti
sostanziali rispetto alla pipeline 1A in termini di prestazioni predittive.
Le metriche di valutazione risultano infatti comparabili, indicando che
l’aumento della complessità del modello non si traduce in un beneficio
significativo nel contesto analizzato. Di conseguenza, la pipeline 1B viene
considerata principalmente come termine di confronto, mentre la pipeline 1A
rappresenta una soluzione più efficiente e interpretabile.
\begin{center}
      \includegraphics[scale=0.8, center]{secondpipeline.png}
\end{center}

\section{Reinforcement Learning}
\subsection{Motivazioni dietro al cambio di approccio}
Sebbene i precedenti approcci si siano dimostrati efficienti
nell'affrontare il problema in esame, hanno anche evidenziato delle problematiche:
\begin{itemize}
    \item il Blackjack è un processo decisionale sequenziale, in cui ogni azione influenza gli stati futuri della partita, tuttavia ciò non viene riflesso con questi approcci;
    \item l'assenza di feedback intermedio;
    \item l’azione ritenuta migliore viene dedotta indirettamente a partire dalle predizioni del modello;
    ciò risulta essere meno adatto a problemi in cui l’obiettivo principale è l’ottimizzazione di una sequenza di decisioni.
\end{itemize}
Inoltre i risultati ottenuti evidenziano che il modello riesce a individuare circa il 60\% dei veri casi positivi. Questo evidenzia una percentuale di falsi negativi fin troppo alta.
Di fatto quindi l'informazione di "Perdita o non Perdita" ed
un insieme di feature forse troppo ridotto, portano il nostro modello a non svolgere propriamente il compito prestabilito.
Rendendoci conto che per la costruzione di un modello che fosse in grado di giocare a Blackjack in maniera efficiente non bastava affrontare il problema come un problema di apprendimento supervisionato, abbiamo deciso di cambiare il nostro approccio.
\subsection{Introduzione al ruolo di Markov}
Parallelamente al corso di \textit{Fondamenti di Intelligenza Artificilae}, uno degli esami che abbiamo seguito era quello di \textit{Musimatica}. All'interno di tale corso è stato introdotto il concetto di "Catena di Markov"\cite{dispensaMusimatica}, come un modello matematico per descrivere sistemi che hanno un'evoluzione casuale guidata da distribuzioni di probabilità. Tale modello possiede due componenti: uno insieme di stati e le probabilità di transizione da uno stato all'altro.
Abbiamo quindi identificato un facile parallelismo col blackjack, ma con nozioni veramente basilari non sarebbe stato possibile andare avanti.
Abbiamo quindi condotto delle ricerche basilari, necessarie per capire se stessimo ancora sbagliando strada. 
Per comprenderne i risultati è quindi necessario dare una serie di definizioni.
\subsubsection{Processo di Markov}
Definiamo un \textbf{Processo di Markov} come un processo che non ha memoria degli stati precedenti, e di conseguenza lo stato futuro è dettato esclusivamente dallo stato attuale.
Di questi processi, quelli che hanno un numero finito (o contabile) di stati sono detti \textbf{Catene di Markov}
\cite{markovChains}.
\subsubsection{Stato di Markov}
Il motivo dietro al quale non si tiene conto degli stati precedenti è che ciascuno stato cattura già le informazioni più rilevanti degli stati precedenti.
Possiamo dare una definizione più formale di stato markoviano tramite questa formulazione:
\textit{Uno stato $S_t$ è detto stato di Markov, se e solo se}
\[
P(S_{t+1} \mid S_1, S_2, \ldots, S_t)=P(S_{t+1} \mid S_t).       \cite{markovState}
\]
\subsubsection{Markov Reward Process}
Un \textbf{Processo di Ricompensa di Markov} 
è una catena di Markov con valori di ricompensa (reward).
Un Processo di Ricompensa di Markov è definito come la quadrupla:
\[
(\mathcal{S}, \mathbf{P}, \mathcal{R}, \gamma),
\]
dove:
\begin{itemize}
  \item $\mathcal{S}$ è un insieme finito di stati;
  \item $\mathbf{P}$ è la matrice di transizione degli stati;
  \item $\mathcal{R}$ è la funzione di ricompensa, tale che
  \[
  \mathcal{R}_s = \mathbb{E}[R_{t+1} \mid S_t = s];
  \]
  \item $\gamma$ è il fattore di sconto, con $\gamma \in [0,1]$.
\end{itemize}
In particolare notiamo questo $\gamma$ che indica quanto valore attribuiamo alle ricompense future rispetto a quelle immediate. Quindi se è uguale a zero ci interessano solo le ricompense immediate, al contrario se è uguale a uno consideriamo le ricompense future quasi allo stesso modo di quelle attuali.
\subsubsection{Markov Decision Process}
Un \textbf{Markov Decision Process} (MDP) è un processo di ricompensa di Markov con azioni. 
È un ambiente in cui tutti gli stati sono Markov, è definito dalla quintupla:
\[
(\mathcal{S}, \mathcal{A}, \mathbf{P}, \mathcal{R}, \gamma),
\]
dove $\mathcal{A}$ è un set finito di azioni a.
Questo particolare tipo di processo è spesso associato ad una policy $\pi$, ovvero una distribuzione delle azioni $a$ dato lo stato $s$:
\[
\pi(a \mid s) = P(A_t = a \mid S_t = s)
\]
L'oobiettivo è quello di trovare una policy ottimale, ovvero una strategia che massimizza la ricompensa cumulativa a lungo termine, e che quindi rispetto alle altre policy produrrà un risultato almeno uguale.
Definiamo in particolare un \textbf{MDP Parzialmente Osservabile} (POMDP) un processo decisionale di Markov con stati nascosti, ovvero composto dalla dalla 7-tupla:
\[
(\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathbf{P}, \mathcal{R}, \mathcal{Z}, \gamma),
\]
dove $\mathcal{O}$  un insieme finito di osservazioni e $\mathcal{Z}$ la funzione di osservazione.

\subsection{Markov ed il Blackjack}
Il problema del Blackjack è stato modellato come un MDP. Lo stato dell’ambiente è definito esclusivamente dalle informazioni osservabili dal giocatore al momento della decisione, ovvero: la somma della propria mano, la presenza di una mano soft, e la carta scoperta del dealer. Sebbene il gioco del Blackjack, nella sua formulazione completa, sia più propriamente descrivibile come un Processo Decisionale Parzialmente Osservabile (POMDP) a causa della dipendenza dall’ordine delle carte nel mazzo, abbiamo assunto una formulazione \textit{puramente} markoviana rispetto allo stato informativo del giocatore, ignorando volontariamente informazioni non osservabili. 
Tale approssimazioni consente quindi di affrontare il problema come un MDP episodico con stati e azioni discrete, con ricompensa associata all’esito finale della mano.
Abbiamo trovato riscontro con tale affermazione\cite{mdpBJ}, ciò ci ha moralmente dato un via libera per il procedere con questo approccio.
\subsection{Scelta del Reinforcement Learning}
Una volta classificato il problema, abbiamo deciso di adottare un approccio di Reinforcement Learning, che viene naturale siccome trattiamo il tutto come un MDP.
Difatto definiamo il Reinforcement Learning come un paradigma di apprendimento automatico in cui un agente impara a prendere decisioni sequenziali interagendo con un ambiente, al fine di massimizzare una misura cumulativa di ricompensa nel lungo periodo.
Più precisamente, l’agente e l’ambiente interagiscono attraverso una sequenza di istanti temporali discreti
\( t = 0, 1, 2, 3, \ldots \).
Per ogni istante \( t \), l’agente riceve una rappresentazione dello stato dell’ambiente,
\( S_t \in \mathcal{S} \), dove \( \mathcal{S} \) è l’insieme di tutti gli stati possibili.
Sulla base di tale informazione, l’agente seleziona un’azione
\( A_t \in \mathcal{A}(S_t) \), dove \( \mathcal{A}(S_t) \) rappresenta l’insieme delle azioni disponibili
nello stato \( S_t \).
All'istante successivo, come conseguenza dell’azione intrapresa, l’agente riceve una ricompensa numerica
\( R_{t+1} \in \mathcal{R} \subset \mathcal{R} \) e si ritrova in un nuovo stato \( S_{t+1} \).
\begin{center}
      \includegraphics[scale=0.6, center]{agent.png}
  \end{center}\cite{stanfordRL}
Il problema adesso è quindi quello di far apprendere una policy che massimizzi il ritorno atteso nel lungo periodo. 
\subsection{On-policy vs Off-policy}
Per comprendere le nostre scelte è necessario prima comprendere una differenza sostanziale tra due tipi di Reinforcement Learning: on-policy ed off-policy.
Nel Reinforcement Learning, gli algoritmi possono essere classificati in base al rapporto tra la
\textbf{policy utilizzata per interagire con l’ambiente}(\textit{comportamentale}) e la \textbf{policy che viene appresa}(\textit{target}).
Un algoritmo è detto \textbf{off-policy} quando la policy comportamentale è diversa dalla policy target.
Questo consente di apprendere una policy ottimale indipendentemente dalla strategia di
esplorazione adottata. 
In questo caso quindi l'agente apprende il valore
delle azioni assumendo che in futuro venga seguita una policy ottimale, indipendentemente dal comportamento esplorativo corrente. Questo consente di convergere più facilmente verso una strategia teoricamente ottimale, ma che potrebbe risultare più \textit{aggressiva}, che potrebbe non tener conto del rischio associato all’esplorazione durante la fase di apprendimento.
Invece l'algoritmo è detto \textbf{on-policy} quando la policy utilizzata per selezionare le azioni
durante l’interazione con l’ambiente coincide con la policy che viene aggiornata e migliorata nel processo di apprendimento. In questo caso, l’agente apprende il valore delle azioni effettivamente intraprese, includendo esplicitamente il comportamento esplorativo.
Nel nostro caso la policy appresa riflette anche il rischio introdotto dalle azioni esplorative. Questo porta l’agente a sviluppare una strategia generalmente più
\textit{conservativa}, penalizzando azioni che, pur essendo ottimali in teoria,
possono risultare rischiose in presenza di esplorazione.
Vale la pena osservare ambo gli approcci, per comprenderne al meglio i trade-off che si ottengono applicandone uno rispetto all'altro nel caso del Blackjack.
Abbiamo quindi selezionato due algoritmi:
\begin{description}
    \item[- Q-learning:] che è un algoritmo di tipo off-policy.
    \item[- Sarsa:] che p un algoritmo di tipo on-policy.
\end{description}
Abbiamo deciso di partire dal primo di questi.
\section{Q-learning}
Il \textbf{Q-learning} è un algoritmo di Reinforcement Learning \textit{off-policy}, introdotto da Christopher J. C. Watkins nel 1989 \cite{watkins1989learning}, conl’obiettivo di apprendere una policy ottimale
per un Processo Decisionale di Markov senza richiedere la conoscenza delle dinamiche dell’ambiente.
\subsection{Definizione formale}
Il Q-learning si basa sull’apprendimento di una funzione di valore stato-azione \( Q(s,a) \), che rappresenta il ritorno atteso cumulativo ottenibile eseguendo l’azione \( a \) nello stato \( s \) e seguendo successivamente
la policy ottimale. 
Formalmente:
\[
Q^*(s,a) = \max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_{t+1}
\;\middle|\; S_0 = s, A_0 = a, \pi \right].
\]
\paragraph{Origine storica}
L’algoritmo è stato proposto da Watkins nel suo lavoro di dottorato come estensione dei metodi di programmazione dinamica a contesti in cui il modello dell’ambiente non è noto. 
La sua formulazione consente di aggiornare le stime di \( Q(s,a) \) ad ogni iterazione utilizzando esclusivamente
esperienza diretta, rendendolo particolarmente adatto a problemi stocastici ed episodici.
\subsection{Funzionamento}
Durante l’interazione con l’ambiente, l’agente osserva lo stato corrente \( s_t \), seleziona un’azione \( a_t \) secondo una politica esplorativa , riceve una ricompensa \( r_{t+1} \) e avanza allo stato successivo \( s_{t+1} \).
La stima della funzione \( Q \) viene aggiornata secondo la regola:
\[
Q(s_t,a_t) \leftarrow Q(s_t,a_t) +
\alpha \left[
r_{t+1} + \gamma \max_{a'} Q(s_{t+1},a') - Q(s_t,a_t)
\right],
\]
dove \( \alpha \in (0,1] \) è il tasso di apprendimento e \( \gamma \in [0,1] \) è il fattore di sconto.
Essendo un algoritmo \textbf{off-policy}, il Q-learning aggiorna le proprie stime
assumendo che, a partire dallo stato successivo, venga seguita una policy ottimale, indipendentemente dall’azione effettivamente scelta durante l’esplorazione.
Questa caratteristica consente all’algoritmo di convergere verso una policy ottimale anche in presenza di comportamenti esplorativi e di fatto rischiosi.
\section{SARSA}
L'algoritmo \textbf{SARSA} (State-Action-Reward-State-Action) è un altro algoritmo di Reinforcement Learning
\textit{on-policy}, introdotto per apprendere una policy direttamente a partire dal comportamento effettivamente adottato dall’agente durante l’interazione con l’ambiente.
\section{Origine e contesto}
SARSA fa la sua prima apparizione nel paper "On-Line Q-Learning Using Connectionist System"\cite{SARSA}, sotto il nome di "Modified Connectionist Q-Learning".
L'algoritmo ha due idee cardine dietro al suo funzionamento:
\begin{description}
    \item[Temporal Difference Learning:] l'agente impara da una differenza temporale tra due stime successive, non dal risultato finale completo, permettendo di aggiornare la stima del valore corrente durante il training.
    \item[Exploration-Exploitation tradeoff:] trovare un buon compromesso tra \textit{esplorazione} (quando l’agente compie azioni che lo portano in nuovi stati e situazioni mai sperimentate prima)  ed \textit{exploitation} ( quando l’agente sfrutta le esperienze passate per ottenere prestazioni migliori nell’ambiente in cui si trova attualmente).
\end{description}\cite{medium1}
\subsection{Definizione e differenza col Q-Learning }
L’algoritmo SARSA apprende una funzione di valore azione-stato \( Q(s,a) \), che rappresenta il ritorno atteso cumulativo ottenibile eseguendo l’azione \( a \) nello stato \( s \) e seguendo la stessa policy utilizzata per l’esplorazione.
Il nome stesso dell'algoritmo deriva dalla sequenza di variabili utilizzate nell’aggiornamento.
A differenza del Q-learning, non assume che l’agente segua una policy ottimale nello stato successivo, rendendolo più adatto a contesti in cui il rischio associato all’esplorazione deve essere tenuto in considerazione.
\subsection{Funzionamento dell’algoritmo}
Durante l’interazione con l’ambiente, l’agente osserva lo stato \( s_t \), sceglie un’azione \( a_t \), riceve una ricompensa \( r_{t+1} \) e transita nello stato successivo \( s_{t+1} \), selezionando quindi una nuova azione \( a_{t+1} \) secondo la stessa policy.
L’aggiornamento della funzione \( Q \) è dato da:
\[
Q(s_t,a_t) \leftarrow Q(s_t,a_t) +
\alpha \left[
r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)
\right].
\]
Essendo un algoritmo \textbf{on-policy}, SARSA apprende una policy che riflette
direttamente il comportamento esplorativo dell’agente,risultando in strategie più conservative rispetto a quelle apprese tramite Q-learning, ed è per questo che abbiamo deciso di usarlo.

\section{Applicazione del Reinforcement Learning}
Occorre infine definire come un'ultima distinzione, fondamentale per comprendere le nostre implementazioni, quella tra \textit{online} ed \textit{offline} reinforcement learning.
Per quanto riguarda \textbf{l'online RL} prevede di simulare interazioni con l'ambiente per apprendere delle policy ottimali. Utilizza di volta in volta l'esperienza immediatamente appresa nelle successive interazioni con l'ambiente ed ha il vantaggio di potersi adattare ai cambiamenti data la continua interazione con esso. Il grande problema di questo approccio è che a causa di tutte le simulazioni che genera con l'ambiente è computazionalmente costoso. Di fatto quindi questo tipo di approccio non necessità di un dataset per l'addestramento.
A differenza del precedente l'\textbf{l'offline RL} mira a creare delle policy efficaci su dati collezionati precedentemente, senza nessuna interazione con l'ambiente. Questo approccio porta con se anche delle sfide importanti: occorre migliorare le policy che si raccolgono dai dati così da stimare anche valori per azioni non presenti nel dataset.

\subsection{Implementazione con Q-Learning}
Occorre approfondire gli elementi che caratterizzano il Q-Learning per poterlo comprendere appieno ed implementarlo. Tra gli elementi più importanti c'è proprio la \textbf{Q table}
Quest'ultima è una tabella che contiene i reward per ogni azione di ogni stato ed è possibile farne il lookup. In particolare ha una riga per ogni stato e una colonna per ogni azione. Inizialmente ogni valore di reward della tabella è inizializzato a zero ma ad ogni esplorazione il valore in questione cambia basandosi sull'equazione già precedentemente discussa. Altri elementi fondamentali nel Q-learning sono i seguenti iperparametri caratterizzanti:
\begin{itemize}
    \item $\alpha$: rate di apprendimento che varia da 0 a 1. Occorre ottimizzare questo parametro per fare in modo che i valori di reward di ogni iterazione della Q table siano conoscenza utile al progredire dell'agente.
    %stesso parametro di catene di markov ma ha accezione diversa, da aggiustare, quanto è lungimirante un agente 
    \item $\gamma$: termine che regola come comportarsi nel corso della ricerca della soluzione. Più vicino si arriva alla soluzione e maggiore sarà la preferenza verso dei reward a breve termine.
    \item $\epsilon$: termine che regola l'esplorazione facendola diminuire nel tempo di modo che la policy appresa migliori.
\end{itemize}
Tutti e tre i parametri quindi col tempo diminuiscono.
\subsection{Implementazione 1 – Q-Learning Online}
La prima implementazione adotta un approccio di Q-learning online, nel quale l'agente apprende interagendo direttamente con un ambiente simulato di Blackjack. 
\subsubsection{Architettura e Implementazione}

L'architettura si compone di due componenti fondamentali: un ambiente di gioco implementato nella classe \texttt{BlackjackEnv} e un agente Q-learning descritto dalla classe \texttt{QLearningAgent}.

L'ambiente simula le dinamiche del gioco utilizzando sei mazzi di carte francesi standard, mescolati casualmente a ogni reset episodico. La gestione del mazzo include un meccanismo di ricreazione automatica quando le carte disponibili si esauriscono durante una mano, garantendo che l'agente possa sempre completare la propria sequenza decisionale senza interruzioni dovute a vincoli di disponibilità. Tale scelta implementativa, sebbene semplifichi la logica di esecuzione, introduce una sottile differenza rispetto alle regole convenzionali del Blackjack nei casinò reali, dove il rimescolamento avviene secondo una penetrazione predefinita del mazzo.

Lo stato dell'ambiente è rappresentato dalla tripla $(s_{\text{player}}, s_{\text{dealer}}, a_{\text{usable}})$, dove $s_{\text{player}}$ indica la somma delle carte del giocatore, $s_{\text{dealer}}$ rappresenta il valore della carta scoperta del dealer, e $a_{\text{usable}}$ è un indicatore booleano della presenza di un asso utilizzabile come 11 senza causare sballamento. Questa rappresentazione dello stato è conforme alla formulazione classica del problema del Blackjack come Markov Decision Process episodico, nel quale lo stato cattura tutta l'informazione rilevante per la decisione ottimale del giocatore.

L'agente implementa l'algoritmo Q-learning con i seguenti iperparametri: $\varepsilon = 0.1$ per la politica $\varepsilon$-greedy, $\alpha = 0.1$ come tasso di apprendimento, e $\gamma = 0.9$ quale fattore di sconto. La funzione di valore azione-stato $Q(s,a)$ è rappresentata mediante una tabella implementata come dizionario Python con inizializzazione ottimistica a zero, che associa a ogni coppia stato-azione un vettore di due valori corrispondenti alle azioni disponibili: \textit{stand} (codificata come 0) e \textit{hit} (codificata come 1). Tale rappresentazione tabulare è appropriata per il Blackjack, dato il numero limitato di stati osservabili. 

\subsubsection{Processo di Apprendimento}

Il processo di training si articola su un totale di un milione di episodi. Ad ogni episodio, l'ambiente viene reinizializzato mediante chiamata al metodo \texttt{reset()}, che ricrea il mazzo di carte e distribuisce la mano iniziale. L'agente seleziona le azioni secondo una strategia $\varepsilon$-greedy durante la fase di training: con probabilità $\varepsilon$ sceglie un'azione casuale per favorire l'esplorazione dello spazio degli stati, mentre con probabilità $1-\varepsilon$ seleziona l'azione con valore Q massimo per lo stato corrente, sfruttando la conoscenza acquisita.

L'aggiornamento della funzione Q segue la regola standard del Q-learning off-policy:
\begin{equation}
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\right]
\end{equation}

dove $s_t$ è lo stato al tempo $t$, $a_t$ è l'azione intrapresa, $r_{t+1}$ è la ricompensa ricevuta, e $s_{t+1}$ rappresenta lo stato successivo. L'operatore $\max$ calcola il valore massimo tra le azioni disponibili nello stato futuro, rendendo l'algoritmo off-policy: l'agente aggiorna le stime assumendo che in futuro seguirà la politica ottimale, indipendentemente dall'azione effettivamente scelta durante l'esplorazione. Questa caratteristica permette all'agente di apprendere una politica più aggressiva e potenzialmente ottimale, anche quando il comportamento esplorativo introduce azioni subottimali.

La struttura delle ricompense è definita come segue: $+1$ per vittoria, $0$ per pareggio, $-1$ per sconfitta. Questo schema di ricompense sparse, concentrate esclusivamente alla fine dell'episodio, rende il problema particolarmente adatto al Q-learning episodico, nel quale l'agente impara a valutare le conseguenze a lungo termine delle proprie azioni iniziali attraverso il meccanismo di propagazione del valore tramite il fattore di sconto $\gamma$.

\subsubsection{Risultati Sperimentali}

Al termine del training su un milione di episodi, la Q-table risultante contiene valori appresi per 280 coppie stato-azione distinte incontrate durante l'interazione con l'ambiente. I risultati finali evidenziano un \textit{win rate} pari a 0.4029 (40.29\%), un \textit{loss rate} di 0.5104 (51.04\%) e un \textit{draw rate} di 0.0867 (8.67\%).

L'analisi della curva di apprendimento, ottenuta mediante media mobile su finestre di 10.000 episodi, mostra una convergenza progressiva della ricompensa media verso valori asintoticamente stabili. Il \textit{win rate} cumulativo si stabilizza intorno a valori che si avvicinano alla soglia di riferimento della strategia di base ottimale del Blackjack, documentata nella letteratura \cite{optimalStrategy}essere approssimativamente 0.42 nelle condizioni regolamentari standard. Questo comportamento conferma che l'agente è in grado di migliorare significativamente le proprie prestazioni rispetto a una strategia puramente casuale, apprendendo una politica che riflette principi decisionali fondamentali del Blackjack, quali la necessità di richiedere carta quando la somma è bassa e di mantenersi quando la somma è prossima a 21.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.00\textwidth]{3a.png}
    \caption{Risultati Implementazione 1 – Online Q-Learning}
    \label{fig:pipeline3-offline}
\end{figure}




\section{Glossario}
\renewcommand{\arraystretch}{1.5} % Aumenta lo spazio verticale tra le righe
\setlength{\arrayrulewidth}{0.5pt}

\noindent
\begin{tabularx}{\textwidth}{>{\bfseries}l X} % Prima colonna in grassetto, seconda estesa
    \rowcolor{Gray!20} % Colore dell'intestazione
    Termine & Definizione \\ \toprule
    
    Data binning & Tecnica di data preprocessing utilizzata per diminuire l'errore osservabile.I dati originali sono ridotti a un piccolo range di intervalli decisi tramite varie metodologie. \\ 
    \midrule
    
    Data leakage & Problema di un modello di ML: il modello è capace di lavorare accuratamente solo in fase di addestramento e non in fase di rilascio. \\ 
    \midrule
 
    \bottomrule
\end{tabularx}

\bibliographystyle{unsrt}
\bibliography{bibliography.bib}
\end{document}