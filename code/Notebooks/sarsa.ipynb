{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0pqeCkGxVUW",
        "outputId": "020603c1-c005-4515-ae13-ab47bd9c5830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completato!\n",
            "Environment pronto\n",
            "Agente SARSA pronto\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "!pip install -q pandas numpy matplotlib seaborn tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Setup completato!\")\n",
        "\n",
        "# Blackjack Environment\n",
        "class BlackjackEnv:\n",
        "    def __init__(self, num_decks=8):\n",
        "        self.num_decks = num_decks\n",
        "        self.reset_deck()\n",
        "\n",
        "    def reset_deck(self):\n",
        "        deck = []\n",
        "        for _ in range(self.num_decks):\n",
        "            for _ in range(4):\n",
        "                deck.extend([11] + list(range(2, 11)) + [10, 10, 10])\n",
        "        random.shuffle(deck)\n",
        "        self.deck = deck\n",
        "\n",
        "    def draw_card(self):\n",
        "        if len(self.deck) < 20:\n",
        "            self.reset_deck()\n",
        "        return self.deck.pop()\n",
        "\n",
        "    def get_hand_value(self, hand):\n",
        "        value = sum(hand)\n",
        "        aces = hand.count(11)\n",
        "        while value > 21 and aces > 0:\n",
        "            value -= 10\n",
        "            aces -= 1\n",
        "        is_soft = (aces > 0 and value <= 21)\n",
        "        return value, is_soft\n",
        "\n",
        "    def is_bust(self, hand):\n",
        "        value, _ = self.get_hand_value(hand)\n",
        "        return value > 21\n",
        "\n",
        "    def dealer_play(self, dealer_hand):\n",
        "        while True:\n",
        "            value, is_soft = self.get_hand_value(dealer_hand)\n",
        "            if value > 21:\n",
        "                break\n",
        "            if value >= 17 and not is_soft:\n",
        "                break\n",
        "            if value == 17 and is_soft:\n",
        "                dealer_hand.append(self.draw_card())\n",
        "            elif value < 17:\n",
        "                dealer_hand.append(self.draw_card())\n",
        "            else:\n",
        "                break\n",
        "        return dealer_hand\n",
        "\n",
        "    def reset(self):\n",
        "        player_hand = [self.draw_card(), self.draw_card()]\n",
        "        dealer_hand = [self.draw_card(), self.draw_card()]\n",
        "        return {\n",
        "            'player_hand': player_hand,\n",
        "            'dealer_hand': dealer_hand,\n",
        "            'dealer_showing': dealer_hand[0]\n",
        "        }\n",
        "\n",
        "    def step(self, state, action):\n",
        "        player_hand = state['player_hand'].copy()\n",
        "        dealer_hand = state['dealer_hand'].copy()\n",
        "        dealer_showing = state['dealer_showing']\n",
        "        done = False\n",
        "        reward = 0\n",
        "        info = {}\n",
        "\n",
        "        if action == 1:  # HIT\n",
        "            player_hand.append(self.draw_card())\n",
        "            if self.is_bust(player_hand):\n",
        "                reward = -1\n",
        "                done = True\n",
        "                info['outcome'] = 'player_bust'\n",
        "            else:\n",
        "                return {\n",
        "                    'player_hand': player_hand,\n",
        "                    'dealer_hand': dealer_hand,\n",
        "                    'dealer_showing': dealer_showing\n",
        "                }, reward, done, info\n",
        "\n",
        "        elif action == 0:  # STAND\n",
        "            done = True\n",
        "            dealer_hand = self.dealer_play(dealer_hand)\n",
        "            player_value, _ = self.get_hand_value(player_hand)\n",
        "            dealer_value, _ = self.get_hand_value(dealer_hand)\n",
        "\n",
        "            if self.is_bust(dealer_hand):\n",
        "                reward = 1\n",
        "                info['outcome'] = 'dealer_bust'\n",
        "            elif player_value > dealer_value:\n",
        "                reward = 1\n",
        "                info['outcome'] = 'player_wins'\n",
        "            elif player_value < dealer_value:\n",
        "                reward = -1\n",
        "                info['outcome'] = 'dealer_wins'\n",
        "            else:\n",
        "                reward = 0\n",
        "                info['outcome'] = 'push'\n",
        "\n",
        "        return {\n",
        "            'player_hand': player_hand,\n",
        "            'dealer_hand': dealer_hand,\n",
        "            'dealer_showing': dealer_showing\n",
        "        }, reward, done, info\n",
        "\n",
        "print(\"Environment pronto\")\n",
        "# SARSA Agent\n",
        "def state_to_tuple(state, env):\n",
        "    player_value, is_soft = env.get_hand_value(state['player_hand'])\n",
        "    return (player_value, int(is_soft), state['dealer_showing'])\n",
        "\n",
        "class SARSAAgent:\n",
        "    def __init__(self, learning_rate=0.1, discount_factor=0.95,\n",
        "                 epsilon=1.0, epsilon_decay=0.9999, epsilon_min=0.01):\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "        self.episode_rewards = []\n",
        "        self.wins = 0\n",
        "        self.losses = 0\n",
        "        self.draws = 0\n",
        "\n",
        "    def get_best_action(self, state, env):\n",
        "        state_key = state_to_tuple(state, env)\n",
        "        if state_key not in self.q_table:\n",
        "            return random.choice([0, 1])\n",
        "        q_values = self.q_table[state_key]\n",
        "        if not q_values:\n",
        "            return random.choice([0, 1])\n",
        "        max_q = max(q_values.values())\n",
        "        best_actions = [a for a, q in q_values.items() if q == max_q]\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "    def choose_action(self, state, env, training=True):\n",
        "        player_value, _ = env.get_hand_value(state['player_hand'])\n",
        "        if player_value >= 21:\n",
        "            return 0\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.choice([0, 1])\n",
        "        else:\n",
        "            return self.get_best_action(state, env)\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state, next_action, env):\n",
        "        state_key = state_to_tuple(state, env)\n",
        "        next_state_key = state_to_tuple(next_state, env)\n",
        "        current_q = self.q_table[state_key][action]\n",
        "        next_q = self.q_table[next_state_key][next_action]\n",
        "        new_q = current_q + self.lr * (reward + self.gamma * next_q - current_q)\n",
        "        self.q_table[state_key][action] = new_q\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def train(self, env, num_episodes=50000, verbose_every=5000):\n",
        "        print(f\"Training SARSA: {num_episodes} episodi...\")\n",
        "        for episode in tqdm(range(num_episodes)):\n",
        "            state = env.reset()\n",
        "            action = self.choose_action(state, env, training=True)\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not done and steps < 50:\n",
        "                next_state, reward, done, info = env.step(state, action)\n",
        "\n",
        "                if done:\n",
        "                    state_key = state_to_tuple(state, env)\n",
        "                    current_q = self.q_table[state_key][action]\n",
        "                    new_q = current_q + self.lr * (reward - current_q)\n",
        "                    self.q_table[state_key][action] = new_q\n",
        "                else:\n",
        "                    next_action = self.choose_action(next_state, env, training=True)\n",
        "                    self.update_q_value(state, action, reward, next_state, next_action, env)\n",
        "                    state = next_state\n",
        "                    action = next_action\n",
        "\n",
        "                episode_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "            self.episode_rewards.append(episode_reward)\n",
        "            if episode_reward > 0:\n",
        "                self.wins += 1\n",
        "            elif episode_reward < 0:\n",
        "                self.losses += 1\n",
        "            else:\n",
        "                self.draws += 1\n",
        "\n",
        "            self.decay_epsilon()\n",
        "\n",
        "            if (episode + 1) % verbose_every == 0:\n",
        "                recent_rewards = self.episode_rewards[-verbose_every:]\n",
        "                avg_reward = np.mean(recent_rewards)\n",
        "                win_rate = self.wins / (episode + 1)\n",
        "                print(f\"\\nEp {episode + 1}: Avg Reward={avg_reward:.3f}, WinRate={win_rate:.3f}, Eps={self.epsilon:.4f}\")\n",
        "\n",
        "        print(f\"\\nâœ“ Training completato!\")\n",
        "        print(f\"  Q-table: {len(self.q_table)} stati\")\n",
        "        print(f\"  Win rate: {self.wins / num_episodes:.4f}\")\n",
        "\n",
        "    def evaluate(self, env, num_episodes=10000):\n",
        "        print(f\"\\nEvaluation su {num_episodes} episodi...\")\n",
        "        wins = 0\n",
        "        losses = 0\n",
        "        draws = 0\n",
        "        total_reward = 0\n",
        "\n",
        "        for episode in tqdm(range(num_episodes)):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while not done and steps < 50:\n",
        "                action = self.choose_action(state, env, training=False)\n",
        "                state, reward, done, info = env.step(state, action)\n",
        "                episode_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "            total_reward += episode_reward\n",
        "            if episode_reward > 0:\n",
        "                wins += 1\n",
        "            elif episode_reward < 0:\n",
        "                losses += 1\n",
        "            else:\n",
        "                draws += 1\n",
        "\n",
        "        win_rate = wins / num_episodes\n",
        "        loss_rate = losses / num_episodes\n",
        "        draw_rate = draws / num_episodes\n",
        "        avg_reward = total_reward / num_episodes\n",
        "\n",
        "        print(f\"\\nRisultati:\")\n",
        "        print(f\"  Win Rate:  {win_rate:.4f}\")\n",
        "        print(f\"  Loss Rate: {loss_rate:.4f}\")\n",
        "        print(f\"  Draw Rate: {draw_rate:.4f}\")\n",
        "        print(f\"  Avg Reward: {avg_reward:.4f}\")\n",
        "\n",
        "        return {'win_rate': win_rate, 'loss_rate': loss_rate, 'draw_rate': draw_rate, 'avg_reward': avg_reward}\n",
        "\n",
        "print(\"Agente SARSA pronto\")"
      ]
    }
  ]
}